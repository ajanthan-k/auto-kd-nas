# adls_group_11: Knowledge distillation with neural architacture search for language models
## Overview

The project is centered around designing an efficient optimization pipeline for knowledge distillation of language models. Unlike traditional KD, which focuses on a one-to-one teacher-to-student model compression, our pipeline uses neural architecture search (NAS) to perform KD on a pool of student models. This allows for discovery of more optimal network structures while also enable us to expand the usability of the KD pipeline to a range of different models.

Our pipeline offers the following improvements compare to traditional KD optimizations:
- s
