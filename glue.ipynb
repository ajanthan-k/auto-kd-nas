{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/12/2025 23:07:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/12/2025 23:07:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/run1/qqp/runs/Mar12_23-07-55_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/run1/qqp/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/run1/qqp/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/12/2025 23:07:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/12/2025 23:07:58 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/12/2025 23:07:58 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/12/2025 23:07:58 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:696] 2025-03-12 23:07:58,361 >> loading configuration file config.json from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-12 23:07:58,363 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"qqp\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-03-12 23:07:58,459 >> loading configuration file config.json from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-12 23:07:58,460 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file vocab.txt from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file tokenizer.json from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file tokenizer_config.json from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-03-12 23:07:58,460 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:696] 2025-03-12 23:07:58,461 >> loading configuration file config.json from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-12 23:07:58,461 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3904] 2025-03-12 23:07:58,517 >> loading weights file model.safetensors from cache at /home/tomyt/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "[INFO|logging.py:343] 2025-03-12 23:07:58,546 >> A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
      "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "[INFO|modeling_utils.py:4878] 2025-03-12 23:07:58,554 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-12 23:07:58,554 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|           | 0/363846 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e12d502673c7cc57.arrow\n",
      "03/12/2025 23:07:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e12d502673c7cc57.arrow\n",
      "Running tokenizer on dataset: 100%|█| 363846/363846 [00:20<00:00, 17523.18 examp\n",
      "Running tokenizer on dataset:   0%|            | 0/40430 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1432d0ba8ca7fc37.arrow\n",
      "03/12/2025 23:08:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1432d0ba8ca7fc37.arrow\n",
      "Running tokenizer on dataset: 100%|█| 40430/40430 [00:02<00:00, 18165.34 example\n",
      "Running tokenizer on dataset:   0%|           | 0/390965 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-593716cac6c255b6.arrow\n",
      "03/12/2025 23:08:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/qqp/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-593716cac6c255b6.arrow\n",
      "Running tokenizer on dataset: 100%|█| 390965/390965 [00:22<00:00, 17565.66 examp\n",
      "03/12/2025 23:08:43 - INFO - __main__ - Sample 335243 of the training set: {'question1': 'How do I crack JEE in a month?', 'question2': 'How do I crack JEE in 4-5 months?', 'label': 0, 'idx': 335243, 'input_ids': [101, 2129, 2079, 1045, 8579, 15333, 2063, 1999, 1037, 3204, 1029, 102, 2129, 2079, 1045, 8579, 15333, 2063, 1999, 1018, 1011, 1019, 2706, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/12/2025 23:08:43 - INFO - __main__ - Sample 58369 of the training set: {'question1': 'Who are the greatest people in the world?', 'question2': 'Can you name some people who have really saved the world?', 'label': 0, 'idx': 58369, 'input_ids': [101, 2040, 2024, 1996, 4602, 2111, 1999, 1996, 2088, 1029, 102, 2064, 2017, 2171, 2070, 2111, 2040, 2031, 2428, 5552, 1996, 2088, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/12/2025 23:08:43 - INFO - __main__ - Sample 13112 of the training set: {'question1': 'What is inside a Camel Crush cigarette?', 'question2': 'Are Camel Crush cigarettes designed to attract teen smokers?', 'label': 0, 'idx': 13112, 'input_ids': [101, 2054, 2003, 2503, 1037, 19130, 10188, 9907, 1029, 102, 2024, 19130, 10188, 15001, 2881, 2000, 9958, 9458, 5610, 2869, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-12 23:08:44,897 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question1, idx, question2. If question1, idx, question2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-12 23:08:44,904 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-12 23:08:44,904 >>   Num examples = 363,846\n",
      "[INFO|trainer.py:2371] 2025-03-12 23:08:44,904 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2372] 2025-03-12 23:08:44,904 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-12 23:08:44,904 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-12 23:08:44,904 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-12 23:08:44,904 >>   Total optimization steps = 34,113\n",
      "[INFO|trainer.py:2378] 2025-03-12 23:08:44,904 >>   Number of trainable parameters = 109,483,778\n",
      "  0%|                                      | 54/34113 [00:13<2:08:34,  4.41it/s]Traceback (most recent call last):\n",
      "  File \"/home/tomyt/adl/mase/run_glue.py\", line 637, in <module>\n",
      "    main()\n",
      "  File \"/home/tomyt/adl/mase/run_glue.py\", line 545, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/transformers/trainer.py\", line 3712, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/accelerate/accelerator.py\", line 2246, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "glue_tasks = ['cola', 'mnli', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "learning_rates = [3e-4, 1e-4, 5e-5, 3e-5]\n",
    "\n",
    "task = 'mrpc'\n",
    "\n",
    "# for task in glue_tasks[2:]:\n",
    "! python run_glue.py \\\n",
    "    --model_name_or_path bert-base-uncased \\\n",
    "    --task_name {task} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 4 \\\n",
    "    --output_dir ./results/run1/{task}/ \\\n",
    "    --report_to none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/15/2025 19:20:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/15/2025 19:20:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/batch_test_kd_last/40/runs/Mar15_19-20-31_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/batch_test_kd_last/40,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/batch_test_kd_last/40,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/15/2025 19:20:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/15/2025 19:20:33 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/15/2025 19:20:33 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/15/2025 19:20:33 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:694] 2025-03-15 19:20:33,448 >> loading configuration file ./batch_test_kd_last/40/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-15 19:20:33,449 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"./batch_test_kd_last/40\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-15 19:20:33,452 >> loading file chat_template.jinja\n",
      "[INFO|modeling_utils.py:3901] 2025-03-15 19:20:33,507 >> loading weights file ./batch_test_kd_last/40/model.safetensors\n",
      "[INFO|modeling_utils.py:4878] 2025-03-15 19:20:33,513 >> Some weights of the model checkpoint at ./batch_test_kd_last/40 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-15 19:20:33,513 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ./batch_test_kd_last/40 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|             | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a70134d0d583817f.arrow\n",
      "03/15/2025 19:20:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a70134d0d583817f.arrow\n",
      "Running tokenizer on dataset: 100%|█| 3668/3668 [00:00<00:00, 10145.83 examples/\n",
      "Running tokenizer on dataset:   0%|              | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-de12bd84a4b5e16a.arrow\n",
      "03/15/2025 19:20:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-de12bd84a4b5e16a.arrow\n",
      "Running tokenizer on dataset: 100%|█| 408/408 [00:00<00:00, 15518.96 examples/s]\n",
      "Running tokenizer on dataset:   0%|             | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4e55c0b2fc570228.arrow\n",
      "03/15/2025 19:20:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4e55c0b2fc570228.arrow\n",
      "Running tokenizer on dataset: 100%|█| 1725/1725 [00:00<00:00, 18253.08 examples/\n",
      "03/15/2025 19:20:34 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [2, 14, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 3534, 4434, 16, 4374, 70, 8930, 10251, 149, 61, 13, 9, 3, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 21, 3534, 8, 6486, 22495, 1748, 20, 14, 495, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/15/2025 19:20:34 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [2, 28297, 3055, 638, 26, 14, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 17, 5948, 25, 946, 183, 2173, 16, 328, 2608, 13, 22, 18, 3389, 776, 19, 176, 8, 2153, 103, 4982, 673, 13165, 13, 9, 3, 3055, 19, 4982, 673, 13165, 13, 22, 18, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 13, 15, 17, 5948, 25, 946, 183, 2173, 16, 2608, 13, 22, 18, 3389, 776, 19, 14, 176, 8, 16481, 6430, 632, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/15/2025 19:20:34 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [2, 1236, 279, 1696, 13, 22, 18, 3033, 2070, 4348, 1022, 18, 5441, 268, 9, 2602, 819, 20, 8737, 240, 9, 2290, 13, 15, 133, 7938, 43, 9105, 1022, 18, 1139, 400, 9, 264, 819, 20, 137, 15, 18886, 9, 2290, 13, 9, 3, 14, 1236, 279, 1696, 13, 22, 18, 3033, 4348, 23, 71, 137, 9, 3283, 819, 13, 15, 54, 713, 9, 1087, 2091, 13, 15, 20, 561, 4536, 9, 4279, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-15 19:20:34,993 >> The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-15 19:20:34,995 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-15 19:20:34,995 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:2371] 2025-03-15 19:20:34,996 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2372] 2025-03-15 19:20:34,996 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-15 19:20:34,996 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-15 19:20:34,996 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-15 19:20:34,996 >>   Total optimization steps = 460\n",
      "[INFO|trainer.py:2378] 2025-03-15 19:20:34,996 >>   Number of trainable parameters = 4,104,706\n",
      " 99%|████████████████████████████████████████▍| 454/460 [00:08<00:00, 54.73it/s][INFO|trainer.py:3910] 2025-03-15 19:20:43,759 >> Saving model checkpoint to ./results/batch_test_kd_last/40/checkpoint-460\n",
      "[INFO|configuration_utils.py:420] 2025-03-15 19:20:43,760 >> Configuration saved in ./results/batch_test_kd_last/40/checkpoint-460/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-15 19:20:43,783 >> Model weights saved in ./results/batch_test_kd_last/40/checkpoint-460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-15 19:20:43,783 >> tokenizer config file saved in ./results/batch_test_kd_last/40/checkpoint-460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-15 19:20:43,784 >> Special tokens file saved in ./results/batch_test_kd_last/40/checkpoint-460/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-03-15 19:20:43,836 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 8.8409, 'train_samples_per_second': 1659.562, 'train_steps_per_second': 52.031, 'train_loss': 0.5085071729577106, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 52.03it/s]\n",
      "[INFO|trainer.py:3910] 2025-03-15 19:20:43,837 >> Saving model checkpoint to ./results/batch_test_kd_last/40\n",
      "[INFO|configuration_utils.py:420] 2025-03-15 19:20:43,838 >> Configuration saved in ./results/batch_test_kd_last/40/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-15 19:20:43,862 >> Model weights saved in ./results/batch_test_kd_last/40/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-15 19:20:43,863 >> tokenizer config file saved in ./results/batch_test_kd_last/40/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-15 19:20:43,863 >> Special tokens file saved in ./results/batch_test_kd_last/40/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =     2087GF\n",
      "  train_loss               =     0.5085\n",
      "  train_runtime            = 0:00:08.84\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =   1659.562\n",
      "  train_steps_per_second   =     52.031\n",
      "03/15/2025 19:20:43 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:917] 2025-03-15 19:20:43,870 >> The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-03-15 19:20:43,871 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-03-15 19:20:43,871 >>   Num examples = 408\n",
      "[INFO|trainer.py:4231] 2025-03-15 19:20:43,871 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 182.97it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6593\n",
      "  eval_combined_score     =     0.7071\n",
      "  eval_f1                 =     0.7549\n",
      "  eval_loss               =     0.6468\n",
      "  eval_runtime            = 0:00:00.32\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =   1259.294\n",
      "  eval_steps_per_second   =    157.412\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "glue_tasks = ['cola', 'mnli', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "learning_rates = [3e-4, 1e-4, 5e-5, 3e-5]\n",
    "\n",
    "task = 'mrpc'\n",
    "\n",
    "! python run_glue.py \\\n",
    "    --model_name_or_path ./batch_test_kd_last/40 \\\n",
    "    --task_name {task} \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 4 \\\n",
    "    --output_dir ./results/batch_test_kd_last/40 \\\n",
    "    --report_to none \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for epoch 0\n",
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/17/2025 10:13:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/17/2025 10:13:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/batch_test_kd_new/0/mrpc/runs/Mar17_10-13-40_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/batch_test_kd_new/0/mrpc,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/batch_test_kd_new/0/mrpc,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/17/2025 10:13:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:13:45 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/17/2025 10:13:45 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:13:45 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:694] 2025-03-17 10:13:45,215 >> loading configuration file ./batch_test_kd_new/0/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-17 10:13:45,216 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"./batch_test_kd_new/0\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,218 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,219 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,219 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,219 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,219 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:13:45,219 >> loading file chat_template.jinja\n",
      "[INFO|modeling_utils.py:3901] 2025-03-17 10:13:45,269 >> loading weights file ./batch_test_kd_new/0/model.safetensors\n",
      "[INFO|modeling_utils.py:4878] 2025-03-17 10:13:45,274 >> Some weights of the model checkpoint at ./batch_test_kd_new/0 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-17 10:13:45,274 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ./batch_test_kd_new/0 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-193e4cf321c10479.arrow\n",
      "03/17/2025 10:13:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-193e4cf321c10479.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-898d8c63c60d9b55.arrow\n",
      "03/17/2025 10:13:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-898d8c63c60d9b55.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ca25e26c8d5c09be.arrow\n",
      "03/17/2025 10:13:45 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ca25e26c8d5c09be.arrow\n",
      "03/17/2025 10:13:45 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [2, 14, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 3534, 4434, 16, 4374, 70, 8930, 10251, 149, 61, 13, 9, 3, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 21, 3534, 8, 6486, 22495, 1748, 20, 14, 495, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:13:45 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [2, 28297, 3055, 638, 26, 14, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 17, 5948, 25, 946, 183, 2173, 16, 328, 2608, 13, 22, 18, 3389, 776, 19, 176, 8, 2153, 103, 4982, 673, 13165, 13, 9, 3, 3055, 19, 4982, 673, 13165, 13, 22, 18, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 13, 15, 17, 5948, 25, 946, 183, 2173, 16, 2608, 13, 22, 18, 3389, 776, 19, 14, 176, 8, 16481, 6430, 632, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:13:45 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [2, 1236, 279, 1696, 13, 22, 18, 3033, 2070, 4348, 1022, 18, 5441, 268, 9, 2602, 819, 20, 8737, 240, 9, 2290, 13, 15, 133, 7938, 43, 9105, 1022, 18, 1139, 400, 9, 264, 819, 20, 137, 15, 18886, 9, 2290, 13, 9, 3, 14, 1236, 279, 1696, 13, 22, 18, 3033, 4348, 23, 71, 137, 9, 3283, 819, 13, 15, 54, 713, 9, 1087, 2091, 13, 15, 20, 561, 4536, 9, 4279, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-17 10:13:46,175 >> The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-17 10:13:46,178 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-17 10:13:46,178 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:2371] 2025-03-17 10:13:46,178 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2372] 2025-03-17 10:13:46,178 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-17 10:13:46,178 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-17 10:13:46,178 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-17 10:13:46,178 >>   Total optimization steps = 460\n",
      "[INFO|trainer.py:2378] 2025-03-17 10:13:46,178 >>   Number of trainable parameters = 4,104,706\n",
      " 99%|████████████████████████████████████████▋| 457/460 [00:08<00:00, 61.34it/s][INFO|trainer.py:3910] 2025-03-17 10:13:54,700 >> Saving model checkpoint to ./results/batch_test_kd_new/0/mrpc/checkpoint-460\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:13:54,701 >> Configuration saved in ./results/batch_test_kd_new/0/mrpc/checkpoint-460/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:13:54,724 >> Model weights saved in ./results/batch_test_kd_new/0/mrpc/checkpoint-460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:13:54,724 >> tokenizer config file saved in ./results/batch_test_kd_new/0/mrpc/checkpoint-460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:13:54,725 >> Special tokens file saved in ./results/batch_test_kd_new/0/mrpc/checkpoint-460/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-03-17 10:13:54,774 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 8.5966, 'train_samples_per_second': 1706.727, 'train_steps_per_second': 53.51, 'train_loss': 0.6192990510360055, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 53.51it/s]\n",
      "[INFO|trainer.py:3910] 2025-03-17 10:13:54,775 >> Saving model checkpoint to ./results/batch_test_kd_new/0/mrpc\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:13:54,776 >> Configuration saved in ./results/batch_test_kd_new/0/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:13:54,797 >> Model weights saved in ./results/batch_test_kd_new/0/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:13:54,798 >> tokenizer config file saved in ./results/batch_test_kd_new/0/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:13:54,798 >> Special tokens file saved in ./results/batch_test_kd_new/0/mrpc/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =     2087GF\n",
      "  train_loss               =     0.6193\n",
      "  train_runtime            = 0:00:08.59\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =   1706.727\n",
      "  train_steps_per_second   =      53.51\n",
      "03/17/2025 10:13:54 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:917] 2025-03-17 10:13:54,804 >> The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, sentence2, idx. If sentence1, sentence2, idx are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-03-17 10:13:54,806 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-03-17 10:13:54,806 >>   Num examples = 408\n",
      "[INFO|trainer.py:4231] 2025-03-17 10:13:54,806 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 152.72it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6789\n",
      "  eval_combined_score     =     0.7404\n",
      "  eval_f1                 =     0.8018\n",
      "  eval_loss               =     0.6081\n",
      "  eval_runtime            = 0:00:00.36\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =   1105.152\n",
      "  eval_steps_per_second   =    138.144\n",
      "Running for epoch 5\n",
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/17/2025 10:13:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/17/2025 10:13:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/batch_test_kd_new/5/mrpc/runs/Mar17_10-13-58_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/batch_test_kd_new/5/mrpc,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/batch_test_kd_new/5/mrpc,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/17/2025 10:14:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:00 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/17/2025 10:14:00 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:00 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:694] 2025-03-17 10:14:00,827 >> loading configuration file ./batch_test_kd_new/5/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-17 10:14:00,829 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"./batch_test_kd_new/5\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:00,831 >> loading file chat_template.jinja\n",
      "[INFO|modeling_utils.py:3901] 2025-03-17 10:14:00,877 >> loading weights file ./batch_test_kd_new/5/model.safetensors\n",
      "[INFO|modeling_utils.py:4878] 2025-03-17 10:14:00,882 >> Some weights of the model checkpoint at ./batch_test_kd_new/5 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-17 10:14:00,882 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ./batch_test_kd_new/5 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f401a6d25add9bb5.arrow\n",
      "03/17/2025 10:14:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f401a6d25add9bb5.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-cc47744233e23a0a.arrow\n",
      "03/17/2025 10:14:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-cc47744233e23a0a.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-639842fe7522e99c.arrow\n",
      "03/17/2025 10:14:00 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-639842fe7522e99c.arrow\n",
      "03/17/2025 10:14:00 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [2, 14, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 3534, 4434, 16, 4374, 70, 8930, 10251, 149, 61, 13, 9, 3, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 21, 3534, 8, 6486, 22495, 1748, 20, 14, 495, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:00 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [2, 28297, 3055, 638, 26, 14, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 17, 5948, 25, 946, 183, 2173, 16, 328, 2608, 13, 22, 18, 3389, 776, 19, 176, 8, 2153, 103, 4982, 673, 13165, 13, 9, 3, 3055, 19, 4982, 673, 13165, 13, 22, 18, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 13, 15, 17, 5948, 25, 946, 183, 2173, 16, 2608, 13, 22, 18, 3389, 776, 19, 14, 176, 8, 16481, 6430, 632, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:00 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [2, 1236, 279, 1696, 13, 22, 18, 3033, 2070, 4348, 1022, 18, 5441, 268, 9, 2602, 819, 20, 8737, 240, 9, 2290, 13, 15, 133, 7938, 43, 9105, 1022, 18, 1139, 400, 9, 264, 819, 20, 137, 15, 18886, 9, 2290, 13, 9, 3, 14, 1236, 279, 1696, 13, 22, 18, 3033, 4348, 23, 71, 137, 9, 3283, 819, 13, 15, 54, 713, 9, 1087, 2091, 13, 15, 20, 561, 4536, 9, 4279, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:01,738 >> The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-17 10:14:01,741 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-17 10:14:01,741 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:2371] 2025-03-17 10:14:01,741 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2372] 2025-03-17 10:14:01,741 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-17 10:14:01,741 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-17 10:14:01,741 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-17 10:14:01,741 >>   Total optimization steps = 460\n",
      "[INFO|trainer.py:2378] 2025-03-17 10:14:01,741 >>   Number of trainable parameters = 4,104,706\n",
      " 99%|████████████████████████████████████████▍| 454/460 [00:08<00:00, 61.93it/s][INFO|trainer.py:3910] 2025-03-17 10:14:10,181 >> Saving model checkpoint to ./results/batch_test_kd_new/5/mrpc/checkpoint-460\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:10,182 >> Configuration saved in ./results/batch_test_kd_new/5/mrpc/checkpoint-460/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:10,203 >> Model weights saved in ./results/batch_test_kd_new/5/mrpc/checkpoint-460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:10,203 >> tokenizer config file saved in ./results/batch_test_kd_new/5/mrpc/checkpoint-460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:10,203 >> Special tokens file saved in ./results/batch_test_kd_new/5/mrpc/checkpoint-460/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-03-17 10:14:10,251 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 8.5103, 'train_samples_per_second': 1724.031, 'train_steps_per_second': 54.052, 'train_loss': 0.542805580470873, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 54.06it/s]\n",
      "[INFO|trainer.py:3910] 2025-03-17 10:14:10,252 >> Saving model checkpoint to ./results/batch_test_kd_new/5/mrpc\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:10,253 >> Configuration saved in ./results/batch_test_kd_new/5/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:10,273 >> Model weights saved in ./results/batch_test_kd_new/5/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:10,274 >> tokenizer config file saved in ./results/batch_test_kd_new/5/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:10,274 >> Special tokens file saved in ./results/batch_test_kd_new/5/mrpc/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =     2087GF\n",
      "  train_loss               =     0.5428\n",
      "  train_runtime            = 0:00:08.51\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =   1724.031\n",
      "  train_steps_per_second   =     54.052\n",
      "03/17/2025 10:14:10 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:10,280 >> The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-03-17 10:14:10,281 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-03-17 10:14:10,281 >>   Num examples = 408\n",
      "[INFO|trainer.py:4231] 2025-03-17 10:14:10,281 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 180.70it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6544\n",
      "  eval_combined_score     =     0.7011\n",
      "  eval_f1                 =     0.7478\n",
      "  eval_loss               =     0.6312\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =   1290.309\n",
      "  eval_steps_per_second   =    161.289\n",
      "Running for epoch 10\n",
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/17/2025 10:14:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/17/2025 10:14:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/batch_test_kd_new/10/mrpc/runs/Mar17_10-14-13_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/batch_test_kd_new/10/mrpc,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/batch_test_kd_new/10/mrpc,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/17/2025 10:14:16 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:16 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/17/2025 10:14:16 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:16 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:694] 2025-03-17 10:14:16,166 >> loading configuration file ./batch_test_kd_new/10/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-17 10:14:16,168 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"./batch_test_kd_new/10\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:16,170 >> loading file chat_template.jinja\n",
      "[INFO|modeling_utils.py:3901] 2025-03-17 10:14:16,215 >> loading weights file ./batch_test_kd_new/10/model.safetensors\n",
      "[INFO|modeling_utils.py:4878] 2025-03-17 10:14:16,220 >> Some weights of the model checkpoint at ./batch_test_kd_new/10 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-17 10:14:16,220 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ./batch_test_kd_new/10 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ef05c5449cbd3ae3.arrow\n",
      "03/17/2025 10:14:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ef05c5449cbd3ae3.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-510c1a4b469e614d.arrow\n",
      "03/17/2025 10:14:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-510c1a4b469e614d.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1a571c6215d026f9.arrow\n",
      "03/17/2025 10:14:16 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1a571c6215d026f9.arrow\n",
      "03/17/2025 10:14:16 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [2, 14, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 3534, 4434, 16, 4374, 70, 8930, 10251, 149, 61, 13, 9, 3, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 21, 3534, 8, 6486, 22495, 1748, 20, 14, 495, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:16 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [2, 28297, 3055, 638, 26, 14, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 17, 5948, 25, 946, 183, 2173, 16, 328, 2608, 13, 22, 18, 3389, 776, 19, 176, 8, 2153, 103, 4982, 673, 13165, 13, 9, 3, 3055, 19, 4982, 673, 13165, 13, 22, 18, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 13, 15, 17, 5948, 25, 946, 183, 2173, 16, 2608, 13, 22, 18, 3389, 776, 19, 14, 176, 8, 16481, 6430, 632, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:16 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [2, 1236, 279, 1696, 13, 22, 18, 3033, 2070, 4348, 1022, 18, 5441, 268, 9, 2602, 819, 20, 8737, 240, 9, 2290, 13, 15, 133, 7938, 43, 9105, 1022, 18, 1139, 400, 9, 264, 819, 20, 137, 15, 18886, 9, 2290, 13, 9, 3, 14, 1236, 279, 1696, 13, 22, 18, 3033, 4348, 23, 71, 137, 9, 3283, 819, 13, 15, 54, 713, 9, 1087, 2091, 13, 15, 20, 561, 4536, 9, 4279, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:17,100 >> The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-17 10:14:17,102 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-17 10:14:17,102 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:2371] 2025-03-17 10:14:17,102 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2372] 2025-03-17 10:14:17,102 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-17 10:14:17,102 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-17 10:14:17,102 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-17 10:14:17,102 >>   Total optimization steps = 460\n",
      "[INFO|trainer.py:2378] 2025-03-17 10:14:17,103 >>   Number of trainable parameters = 4,104,706\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 60.79it/s][INFO|trainer.py:3910] 2025-03-17 10:14:25,464 >> Saving model checkpoint to ./results/batch_test_kd_new/10/mrpc/checkpoint-460\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:25,465 >> Configuration saved in ./results/batch_test_kd_new/10/mrpc/checkpoint-460/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:25,485 >> Model weights saved in ./results/batch_test_kd_new/10/mrpc/checkpoint-460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:25,485 >> tokenizer config file saved in ./results/batch_test_kd_new/10/mrpc/checkpoint-460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:25,485 >> Special tokens file saved in ./results/batch_test_kd_new/10/mrpc/checkpoint-460/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-03-17 10:14:25,535 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 8.4326, 'train_samples_per_second': 1739.909, 'train_steps_per_second': 54.55, 'train_loss': 0.532593237835428, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 54.55it/s]\n",
      "[INFO|trainer.py:3910] 2025-03-17 10:14:25,536 >> Saving model checkpoint to ./results/batch_test_kd_new/10/mrpc\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:25,537 >> Configuration saved in ./results/batch_test_kd_new/10/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:25,560 >> Model weights saved in ./results/batch_test_kd_new/10/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:25,561 >> tokenizer config file saved in ./results/batch_test_kd_new/10/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:25,561 >> Special tokens file saved in ./results/batch_test_kd_new/10/mrpc/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =     2087GF\n",
      "  train_loss               =     0.5326\n",
      "  train_runtime            = 0:00:08.43\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =   1739.909\n",
      "  train_steps_per_second   =      54.55\n",
      "03/17/2025 10:14:25 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:25,568 >> The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-03-17 10:14:25,569 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-03-17 10:14:25,569 >>   Num examples = 408\n",
      "[INFO|trainer.py:4231] 2025-03-17 10:14:25,569 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 183.80it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6887\n",
      "  eval_combined_score     =     0.7332\n",
      "  eval_f1                 =     0.7776\n",
      "  eval_loss               =     0.6156\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =   1287.579\n",
      "  eval_steps_per_second   =    160.947\n",
      "Running for epoch 15\n",
      "/home/tomyt/anaconda3/envs/mase/lib/python3.11/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5080 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_37 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "03/17/2025 10:14:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/17/2025 10:14:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./results/batch_test_kd_new/15/mrpc/runs/Mar17_10-14-29_DESKTOP-4BJ3NOE,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results/batch_test_kd_new/15/mrpc,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results/batch_test_kd_new/15/mrpc,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/17/2025 10:14:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:31 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/17/2025 10:14:31 - INFO - datasets.builder - Found cached dataset glue (/home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/17/2025 10:14:31 - INFO - datasets.info - Loading Dataset info from /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:694] 2025-03-17 10:14:31,398 >> loading configuration file ./batch_test_kd_new/15/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-03-17 10:14:31,400 >> Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"./batch_test_kd_new/15\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,401 >> loading file spiece.model\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,401 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,402 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,402 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,402 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-03-17 10:14:31,402 >> loading file chat_template.jinja\n",
      "[INFO|modeling_utils.py:3901] 2025-03-17 10:14:31,447 >> loading weights file ./batch_test_kd_new/15/model.safetensors\n",
      "[INFO|modeling_utils.py:4878] 2025-03-17 10:14:31,451 >> Some weights of the model checkpoint at ./batch_test_kd_new/15 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4890] 2025-03-17 10:14:31,451 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ./batch_test_kd_new/15 and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d302fc064e550cb7.arrow\n",
      "03/17/2025 10:14:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d302fc064e550cb7.arrow\n",
      "Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7ffefd3bfe074863.arrow\n",
      "03/17/2025 10:14:31 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7ffefd3bfe074863.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d19677a9283aaa19.arrow\n",
      "03/17/2025 10:14:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/tomyt/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-d19677a9283aaa19.arrow\n",
      "Running tokenizer on dataset: 100%|█| 1725/1725 [00:00<00:00, 12360.19 examples/\n",
      "03/17/2025 10:14:31 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [2, 14, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 3534, 4434, 16, 4374, 70, 8930, 10251, 149, 61, 13, 9, 3, 8678, 46, 658, 71, 29, 9601, 18, 70, 8930, 66, 610, 149, 589, 661, 2553, 13, 15, 1876, 21, 3534, 8, 6486, 22495, 1748, 20, 14, 495, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:31 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [2, 28297, 3055, 638, 26, 14, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 17, 5948, 25, 946, 183, 2173, 16, 328, 2608, 13, 22, 18, 3389, 776, 19, 176, 8, 2153, 103, 4982, 673, 13165, 13, 9, 3, 3055, 19, 4982, 673, 13165, 13, 22, 18, 3484, 8, 18614, 283, 50, 21, 5408, 2935, 26, 8485, 13, 15, 17, 5948, 25, 946, 183, 2173, 16, 2608, 13, 22, 18, 3389, 776, 19, 14, 176, 8, 16481, 6430, 632, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/17/2025 10:14:31 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [2, 1236, 279, 1696, 13, 22, 18, 3033, 2070, 4348, 1022, 18, 5441, 268, 9, 2602, 819, 20, 8737, 240, 9, 2290, 13, 15, 133, 7938, 43, 9105, 1022, 18, 1139, 400, 9, 264, 819, 20, 137, 15, 18886, 9, 2290, 13, 9, 3, 14, 1236, 279, 1696, 13, 22, 18, 3033, 4348, 23, 71, 137, 9, 3283, 819, 13, 15, 54, 713, 9, 1087, 2091, 13, 15, 20, 561, 4536, 9, 4279, 13, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:32,751 >> The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2369] 2025-03-17 10:14:32,753 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-03-17 10:14:32,753 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:2371] 2025-03-17 10:14:32,753 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:2372] 2025-03-17 10:14:32,753 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2375] 2025-03-17 10:14:32,753 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-03-17 10:14:32,753 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2377] 2025-03-17 10:14:32,753 >>   Total optimization steps = 460\n",
      "[INFO|trainer.py:2378] 2025-03-17 10:14:32,753 >>   Number of trainable parameters = 4,104,706\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 57.43it/s][INFO|trainer.py:3910] 2025-03-17 10:14:41,252 >> Saving model checkpoint to ./results/batch_test_kd_new/15/mrpc/checkpoint-460\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:41,253 >> Configuration saved in ./results/batch_test_kd_new/15/mrpc/checkpoint-460/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:41,273 >> Model weights saved in ./results/batch_test_kd_new/15/mrpc/checkpoint-460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:41,274 >> tokenizer config file saved in ./results/batch_test_kd_new/15/mrpc/checkpoint-460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:41,274 >> Special tokens file saved in ./results/batch_test_kd_new/15/mrpc/checkpoint-460/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-03-17 10:14:41,325 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 8.5718, 'train_samples_per_second': 1711.65, 'train_steps_per_second': 53.664, 'train_loss': 0.5311594424040421, 'epoch': 4.0}\n",
      "100%|█████████████████████████████████████████| 460/460 [00:08<00:00, 53.66it/s]\n",
      "[INFO|trainer.py:3910] 2025-03-17 10:14:41,326 >> Saving model checkpoint to ./results/batch_test_kd_new/15/mrpc\n",
      "[INFO|configuration_utils.py:420] 2025-03-17 10:14:41,326 >> Configuration saved in ./results/batch_test_kd_new/15/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2988] 2025-03-17 10:14:41,351 >> Model weights saved in ./results/batch_test_kd_new/15/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-03-17 10:14:41,351 >> tokenizer config file saved in ./results/batch_test_kd_new/15/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-03-17 10:14:41,351 >> Special tokens file saved in ./results/batch_test_kd_new/15/mrpc/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               =     2087GF\n",
      "  train_loss               =     0.5312\n",
      "  train_runtime            = 0:00:08.57\n",
      "  train_samples            =       3668\n",
      "  train_samples_per_second =    1711.65\n",
      "  train_steps_per_second   =     53.664\n",
      "03/17/2025 10:14:41 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:917] 2025-03-17 10:14:41,358 >> The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:4226] 2025-03-17 10:14:41,359 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4228] 2025-03-17 10:14:41,359 >>   Num examples = 408\n",
      "[INFO|trainer.py:4231] 2025-03-17 10:14:41,359 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 181.73it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.6618\n",
      "  eval_combined_score     =     0.7077\n",
      "  eval_f1                 =     0.7536\n",
      "  eval_loss               =     0.6455\n",
      "  eval_runtime            = 0:00:00.31\n",
      "  eval_samples            =        408\n",
      "  eval_samples_per_second =   1288.582\n",
      "  eval_steps_per_second   =    161.073\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "glue_tasks = ['cola', 'mnli', 'mrpc', 'qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "learning_rates = [3e-4, 1e-4, 5e-5, 3e-5]\n",
    "\n",
    "task = 'mrpc'\n",
    "\n",
    "test_name = 'batch_test_kd_new'\n",
    "\n",
    "step = 5  # Run every 5 epochs\n",
    "epochs = range(0, 16, step)\n",
    "for epoch in epochs:\n",
    "    output_dir = f'./{test_name}/{epoch}'\n",
    "    result_dir = f'./results/{test_name}/{epoch}/{task}'\n",
    "    \n",
    "    if not os.path.exists(result_dir):\n",
    "        print(f\"Running for epoch {epoch}\")\n",
    "        ! python run_glue.py \\\n",
    "            --model_name_or_path {output_dir} \\\n",
    "            --task_name {task} \\\n",
    "            --do_train \\\n",
    "            --do_eval \\\n",
    "            --per_device_train_batch_size 32 \\\n",
    "            --learning_rate 1e-4 \\\n",
    "            --num_train_epochs 4 \\\n",
    "            --output_dir {result_dir} \\\n",
    "            --report_to none\n",
    "    else:\n",
    "        print(f\"Results for epoch {epoch} already exist, skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "epochs = epochs\n",
    "test_name = test_name\n",
    "\n",
    "for epoch in epochs:\n",
    "    result_file = f'./results/{test_name}/{epoch}/{task}/eval_results.json'\n",
    "    if os.path.exists(result_file):\n",
    "        with open(result_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            accuracies.append(results['eval_accuracy'])\n",
    "            f1_scores.append(results['eval_f1'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs[:len(accuracies)], accuracies, 'b-o', label='Accuracy')\n",
    "plt.plot(epochs[:len(f1_scores)], f1_scores, 'r-o', label='F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('MRPC Performance vs Training Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
